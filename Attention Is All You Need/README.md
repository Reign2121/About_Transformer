## Attention Is All You Need (feat. Transformer)

__________

<br>

- [Review](https://seollane22.tistory.com/20)

</br>

<br>

"Attention is all you need", 이 논문은 기존 seq to seq 모델의 한계점을 보완하고 성능을 크게 개선한 Transformer 모델의 등장을 알린 기념비적인 논문이다.

현재 NLP와 같이 seq to seq 형태의 데이터를 처리하는 태스크에서는 트랜스포머가 주류를 차지하고 있으며, 

시계열 분석에서도 그 활용성을 높이려는 연구가 활발하게 진행되고 있다.

즉, 새로운 논문에서 제시되는 "State of the art"(sota)모델들의 대부분이 이 트랜스포머에 바탕을 두고 있는 것이다.

때문에 결국 이 논문에서 제시한 Attention 알고리즘과 트랜스포머 모델을 제대로 이해하는 것은 최신 트렌드를 이해하고 더 나아가 새로운 연구 기회를 포착하는 데 있어서 매우 중요한 첫 걸음일 것이다.


</br>

