"Attention is all you need", 이 논문은 기존 seq to seq 모델의 한계점을 보완하고 성능을 크게 개선한 Transformer 모델의 등장을 알린 기념비적인 논문이다.

현재 NLP와 같이 seq to seq 형태의 데이터를 처리하는 태스크에서는 트랜스포머가 주류를 차지하고 있으며, 시계열 분석에서도 그 활용성을 높이려는 연구가 활발하게 진행되고 있다.

즉, 새로운 논문에서 제시되는 "State of the art"(sota)모델들의 대부분이 이 트랜스포머에 바탕을 두고 있는 것이다.

때문에 결국 이 논문에서 제시한 Attention 알고리즘과 트랜스포머 모델을 제대로 이해하는 것은 최신 트렌드를 이해하고 더 나아가 새로운 연구 기회를 포착하는 데 있어서 매우 중요한 첫 걸음일 것이다.


- [(중략) 더보기](https://seollane22.tistory.com/20)

__________


마치며
 

오늘날 AI의 트랜드를 논할 때 이 트랜스포머를 빼놓을 수 없을 것이다. Sequential task, 특히 번역 작업에서 트랜스포머의 파급력은 이미 널리 알려졌기 때문에 더 강조하는 것은 의미가 없어보일 정도이다. 

여전히 이 트랜스포머를 베이스로 한 응용 모델들이 활발하게 연구되고 있으며 현업에서도 그 성능을 인정받았다고 할 수 있다.

그러나 필자는 이러한 NLP에도 관심이 있지만, 같은 sequential task인 시계열 데이터 분석에서 트랜스포머가 어떤 역할을 하게 될 지에 대해 가장 큰 관심이 있다.

비즈니스 도메인을 지향하는 학생으로서 시계열 데이터 분석이 비즈니스에서 큰 역할을 할 수 있다는 것을 잘 알고 있다.  

어떻게 보면 장기 종속성(long dependence) 문제에 가장 치명적인 것이 바로 시계열 분석이기 때문에, 시계열 분석 전문가들에게 이 트랜스포머의 등장은 큰 반가움으로 다가왔을 지도 모른다.

2023년 이 리뷰를 작성하는 시점에서 바라보면, 2017년 이 논문을 통해 트랜스포머가 제안된 뒤, 시계열 분석에서도 이를 적용하려는 시도가 당연히 있어왔다는 것을 알 수 있다.

사실 더 정확하게 설명한다면, 시간이 지나 기본적인 트랜스포머가 가지는 한계점들을 규명하고, 이를 개선함과 동시에 시계열 데이터가 가지는 특징을 모델에 녹여내려는 시도들이 이어지고 있다. 

실제로 연구 결과 트랜스포머의 응용을 통해 여러 개선이 이루어졌고, 시계열 분석이 가지는 자기상관성, 추세/계절/순환 변동 등과 같은 고유한 특징들을 고려한 응용 모델이 발표되기도 했다.

이러한 최신 트렌드를 이해하기 위해서는 결국 기초라고 할 수 있는 "기본 트랜스포머, vanilla 트랜스포머"를 제대로 이해하고 있어야한다는 점에서, Attention is all you need 이 논문을 가장 먼저 리뷰
하는 것에 큰 의미가 있을 것이라고 생각한다.

 

다음 포스트에서는 앞서 언급했듯이 시계열 분석에서 트랜스포머가 어떻게 응용되어 왔는 지를 정리한 논문을 리뷰해보고자 한다.
