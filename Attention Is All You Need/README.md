## Attention Is All You Need (feat. Transformer)

<br>

- [Review](https://seollane22.tistory.com/20)

</br>

<br>

오늘날 AI의 트랜드를 논할 때 이 트랜스포머를 빼놓을 수 없을 것이다. Sequential task, 특히 번역 작업에서 트랜스포머의 파급력은 이미 널리 알려졌기 때문에 더 강조하는 것은 의미가 없어보일 정도이다. 

여전히 이 트랜스포머를 베이스로 한 응용 모델들이 활발하게 연구되고 있으며 현업에서도 그 성능을 인정받았다고 할 수 있다.

그러나 필자는 이러한 NLP에도 관심이 있지만, 같은 sequential task인 시계열 데이터 분석에서 트랜스포머가 어떤 역할을 하게 될 지에 대해 가장 큰 관심이 있다.

비즈니스 도메인을 지향하는 학생으로서 시계열 데이터 분석이 비즈니스에서 큰 역할을 할 수 있다는 것을 잘 알고 있다.  

어떻게 보면 장기 종속성(long dependence) 문제에 가장 치명적인 것이 바로 시계열 분석이기 때문에, 시계열 분석 전문가들에게 이 트랜스포머의 등장은 큰 반가움으로 다가왔을 지도 모른다.

2023년 이 리뷰를 작성하는 시점에서 바라보면, 2017년 이 논문을 통해 트랜스포머가 제안된 뒤, 시계열 분석에서도 이를 적용하려는 시도가 당연히 있어왔다는 것을 알 수 있다.

사실 더 정확하게 설명한다면, 시간이 지나 기본적인 트랜스포머가 가지는 한계점들을 규명하고, 이를 개선함과 동시에 시계열 데이터가 가지는 특징을 모델에 녹여내려는 시도들이 이어지고 있다. 

실제로 연구 결과 트랜스포머의 응용을 통해 여러 개선이 이루어졌고, 시계열 분석이 가지는 자기상관성, 추세/계절/순환 변동 등과 같은 고유한 특징들을 고려한 응용 모델이 발표되기도 했다.

이러한 최신 트렌드를 이해하기 위해서는 결국 기초라고 할 수 있는 "기본 트랜스포머, vanilla 트랜스포머"를 제대로 이해하고 있어야한다는 점에서, Attention is all you need 이 논문을 가장 먼저 리뷰
하는 것에 큰 의미가 있을 것이라고 생각한다.

 

다음 포스트에서는 앞서 언급했듯이 시계열 분석에서 트랜스포머가 어떻게 응용되어 왔는 지를 정리한 논문을 리뷰해보고자 한다.
</br>

